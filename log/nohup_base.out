ssh://bureaux@180.169.131.147:22/home/bureaux/miniconda3/envs/Keras-base/bin/python -u /home/bureaux/.pycharm_helpers/pydev/pydevconsole.py --mode=server
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/home/bureaux/Projects/GPLinker', '/home/bureaux/Projects/GPLinker'])
Python 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59)
Type 'copyright', 'credits' or 'license' for more information
IPython 7.16.1 -- An enhanced Interactive Python. Type '?' for help.
runfile('/home/bureaux/Projects/GPLinker/train.py', wdir='/home/bureaux/Projects/GPLinker')
PyDev console: using IPython 7.16.1
Python 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59)
[GCC 7.5.0] on linux
Using TensorFlow backend.
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.
2022-04-09 12:26:59.785503: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2022-04-09 12:26:59.795381: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2022-04-09 12:27:00.308555: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555c386b8550 executing computations on platform CUDA. Devices:
2022-04-09 12:27:00.308625: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Graphics Device, Compute Capability 7.0
2022-04-09 12:27:00.312821: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-04-09 12:27:00.315463: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555c387f92f0 executing computations on platform Host. Devices:
2022-04-09 12:27:00.315509: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-04-09 12:27:00.317294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: Graphics Device major: 7 minor: 0 memoryClockRate(GHz): 1.597
pciBusID: 0000:3b:00.0
2022-04-09 12:27:00.317719: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2022-04-09 12:27:00.320021: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2022-04-09 12:27:00.322153: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2022-04-09 12:27:00.322640: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2022-04-09 12:27:00.325422: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2022-04-09 12:27:00.327619: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2022-04-09 12:27:00.334037: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2022-04-09 12:27:00.337203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2022-04-09 12:27:00.337274: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2022-04-09 12:27:00.339777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-04-09 12:27:00.339810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2022-04-09 12:27:00.339836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2022-04-09 12:27:00.343483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30458 MB memory) -> physical GPU (device: 0, name: Graphics Device, pci bus id: 0000:3b:00.0, compute capability: 7.0)
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:2403: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
Input-Token (InputLayer)        (None, None)         0
__________________________________________________________________________________________________
Input-Segment (InputLayer)      (None, None)         0
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    9216000     Input-Token[0][0]
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]
                                                                 Embedding-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Token-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    0           Embedding-Dropout[0][0]
__________________________________________________________________________________________________
Embedding-Rotary-Position (Sinu (None, None, 64)     0           Embedding-Norm[0][0]
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2359296     Embedding-Norm[0][0]
                                                                 Embedding-Norm[0][0]
                                                                 Embedding-Norm[0][0]
                                                                 Embedding-Rotary-Position[0][0]
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Norm[0][0]
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4718592     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    0           Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2359296     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Embedding-Rotary-Position[0][0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4718592     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    0           Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2359296     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Embedding-Rotary-Position[0][0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4718592     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    0           Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2359296     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Embedding-Rotary-Position[0][0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4718592     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    0           Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2359296     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Embedding-Rotary-Position[0][0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4718592     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    0           Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2359296     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Embedding-Rotary-Position[0][0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4718592     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    0           Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2359296     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Embedding-Rotary-Position[0][0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4718592     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    0           Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2359296     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Embedding-Rotary-Position[0][0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4718592     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    0           Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2359296     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Embedding-Rotary-Position[0][0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4718592     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    0           Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2359296     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Embedding-Rotary-Position[0][0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4718592     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    0           Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2359296     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Embedding-Rotary-Position[0][0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4718592     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    0           Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2359296     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Embedding-Rotary-Position[0][0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4718592     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    0           Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
global_pointer_1 (GlobalPointer (None, 2, None, None 196864      Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
global_pointer_2 (GlobalPointer (None, 53, None, Non 5216896     Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
global_pointer_3 (GlobalPointer (None, 53, None, Non 5216896     Transformer-11-FeedForward-Norm[0
==================================================================================================
Total params: 104,782,848
Trainable params: 104,782,848
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/999
2022-04-09 12:27:29.059157: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
449/449 [==============================] - 609s 1s/step - loss: 91.8303 - global_pointer_1_loss: 10.6130 - global_pointer_2_loss: 42.6791 - global_pointer_3_loss: 38.5381
f1: 0.00000, precision: 1.00000, recall: 0.00000: : 3585it [01:36, 37.31it/s]
best f1: 0.00000
Epoch 2/999
449/449 [==============================] - 599s 1s/step - loss: 20.8080 - global_pointer_1_loss: 4.7649 - global_pointer_2_loss: 8.1455 - global_pointer_3_loss: 7.8975
f1: 0.00376, precision: 0.95238, recall: 0.00188: : 3585it [01:28, 40.33it/s]
best f1: 0.00376
Epoch 3/999
449/449 [==============================] - 595s 1s/step - loss: 17.3179 - global_pointer_1_loss: 4.2940 - global_pointer_2_loss: 6.5926 - global_pointer_3_loss: 6.4313
f1: 0.28070, precision: 0.81374, recall: 0.16960: : 3585it [01:32, 38.90it/s]
best f1: 0.28070
Epoch 4/999
449/449 [==============================] - 600s 1s/step - loss: 15.7523 - global_pointer_1_loss: 4.0048 - global_pointer_2_loss: 5.9523 - global_pointer_3_loss: 5.7952
f1: 0.43984, precision: 0.73997, recall: 0.31292: : 3585it [01:27, 41.18it/s]
best f1: 0.43984
Epoch 5/999
449/449 [==============================] - 607s 1s/step - loss: 14.7256 - global_pointer_1_loss: 3.7923 - global_pointer_2_loss: 5.5312 - global_pointer_3_loss: 5.4021
f1: 0.51451, precision: 0.69990, recall: 0.40677: : 3585it [01:29, 40.18it/s]
best f1: 0.51451
Epoch 6/999
449/449 [==============================] - 601s 1s/step - loss: 13.9362 - global_pointer_1_loss: 3.6260 - global_pointer_2_loss: 5.2182 - global_pointer_3_loss: 5.0920
f1: 0.54733, precision: 0.68073, recall: 0.45765: : 3585it [01:29, 40.24it/s]
best f1: 0.54733
Epoch 7/999
449/449 [==============================] - 604s 1s/step - loss: 13.2227 - global_pointer_1_loss: 3.4608 - global_pointer_2_loss: 4.9264 - global_pointer_3_loss: 4.8356
f1: 0.56510, precision: 0.66607, recall: 0.49072: : 3585it [01:31, 39.03it/s]
best f1: 0.56510
Epoch 8/999
449/449 [==============================] - 599s 1s/step - loss: 12.5782 - global_pointer_1_loss: 3.2967 - global_pointer_2_loss: 4.6920 - global_pointer_3_loss: 4.5895
f1: 0.58013, precision: 0.66235, recall: 0.51607: : 3585it [01:21, 44.15it/s]
best f1: 0.58013
Epoch 9/999
449/449 [==============================] - 606s 1s/step - loss: 12.0656 - global_pointer_1_loss: 3.1700 - global_pointer_2_loss: 4.4915 - global_pointer_3_loss: 4.4042
f1: 0.58858, precision: 0.65763, recall: 0.53265: : 3585it [01:32, 38.94it/s]
best f1: 0.58858
Epoch 10/999
449/449 [==============================] - 603s 1s/step - loss: 11.5667 - global_pointer_1_loss: 3.0488 - global_pointer_2_loss: 4.3058 - global_pointer_3_loss: 4.2121
f1: 0.59549, precision: 0.65520, recall: 0.54575: : 3585it [01:24, 42.31it/s]
best f1: 0.59549
Epoch 11/999
449/449 [==============================] - 604s 1s/step - loss: 10.9723 - global_pointer_1_loss: 2.9010 - global_pointer_2_loss: 4.0705 - global_pointer_3_loss: 4.0008
f1: 0.60167, precision: 0.65288, recall: 0.55790: : 3585it [01:33, 38.24it/s]
best f1: 0.60167
Epoch 12/999
449/449 [==============================] - 595s 1s/step - loss: 10.5138 - global_pointer_1_loss: 2.7904 - global_pointer_2_loss: 3.9046 - global_pointer_3_loss: 3.8188
f1: 0.60609, precision: 0.65404, recall: 0.56468: : 3585it [01:30, 39.40it/s]
best f1: 0.60609
Epoch 13/999
449/449 [==============================] - 602s 1s/step - loss: 10.0708 - global_pointer_1_loss: 2.6649 - global_pointer_2_loss: 3.7441 - global_pointer_3_loss: 3.6617
f1: 0.61177, precision: 0.65398, recall: 0.57467: : 3585it [01:22, 43.22it/s]
best f1: 0.61177
Epoch 14/999
449/449 [==============================] - 598s 1s/step - loss: 9.7200 - global_pointer_1_loss: 2.5768 - global_pointer_2_loss: 3.6066 - global_pointer_3_loss: 3.5366
f1: 0.61555, precision: 0.65460, recall: 0.58089: : 3585it [01:34, 38.04it/s]
best f1: 0.61555
Epoch 15/999
449/449 [==============================] - 602s 1s/step - loss: 9.2016 - global_pointer_1_loss: 2.4460 - global_pointer_2_loss: 3.4088 - global_pointer_3_loss: 3.3469
f1: 0.61964, precision: 0.65788, recall: 0.58560: : 3585it [01:29, 39.92it/s]
best f1: 0.61964
Epoch 16/999
449/449 [==============================] - 607s 1s/step - loss: 8.7609 - global_pointer_1_loss: 2.3347 - global_pointer_2_loss: 3.2425 - global_pointer_3_loss: 3.1837
f1: 0.62223, precision: 0.65850, recall: 0.58975: : 3585it [01:29, 40.10it/s]
best f1: 0.62223
Epoch 17/999
449/449 [==============================] - 597s 1s/step - loss: 8.4217 - global_pointer_1_loss: 2.2444 - global_pointer_2_loss: 3.1118 - global_pointer_3_loss: 3.0654
f1: 0.62493, precision: 0.66044, recall: 0.59305: : 3585it [01:29, 40.05it/s]
best f1: 0.62493
Epoch 18/999
449/449 [==============================] - 608s 1s/step - loss: 8.0688 - global_pointer_1_loss: 2.1552 - global_pointer_2_loss: 2.9872 - global_pointer_3_loss: 2.9264
f1: 0.62393, precision: 0.65671, recall: 0.59427: : 3585it [01:32, 38.66it/s]
Early stop count 1/5
best f1: 0.62493
Epoch 19/999
449/449 [==============================] - 602s 1s/step - loss: 7.7031 - global_pointer_1_loss: 2.0531 - global_pointer_2_loss: 2.8549 - global_pointer_3_loss: 2.7951
f1: 0.62490, precision: 0.65643, recall: 0.59625: : 3585it [01:33, 38.34it/s]
Early stop count 2/5
best f1: 0.62493
Epoch 20/999
449/449 [==============================] - 596s 1s/step - loss: 7.4026 - global_pointer_1_loss: 1.9837 - global_pointer_2_loss: 2.7245 - global_pointer_3_loss: 2.6944
f1: 0.62733, precision: 0.65906, recall: 0.59851: : 3585it [01:30, 39.70it/s]
best f1: 0.62733
Epoch 21/999
449/449 [==============================] - 600s 1s/step - loss: 7.0816 - global_pointer_1_loss: 1.8927 - global_pointer_2_loss: 2.6223 - global_pointer_3_loss: 2.5666
f1: 0.62760, precision: 0.65852, recall: 0.59945: : 3585it [01:33, 38.38it/s]
best f1: 0.62760
Epoch 22/999
449/449 [==============================] - 598s 1s/step - loss: 6.7806 - global_pointer_1_loss: 1.8235 - global_pointer_2_loss: 2.5069 - global_pointer_3_loss: 2.4501
f1: 0.62737, precision: 0.65835, recall: 0.59917: : 3585it [01:27, 41.21it/s]
Early stop count 1/5
best f1: 0.62760
Epoch 23/999
449/449 [==============================] - 604s 1s/step - loss: 6.5185 - global_pointer_1_loss: 1.7519 - global_pointer_2_loss: 2.3997 - global_pointer_3_loss: 2.3669
f1: 0.62700, precision: 0.65743, recall: 0.59927: : 3585it [01:36, 37.15it/s]
Early stop count 2/5
best f1: 0.62760
Epoch 24/999
449/449 [==============================] - 600s 1s/step - loss: 6.2605 - global_pointer_1_loss: 1.6979 - global_pointer_2_loss: 2.3056 - global_pointer_3_loss: 2.2570
f1: 0.62789, precision: 0.65894, recall: 0.59964: : 3585it [01:29, 40.28it/s]
best f1: 0.62789
Epoch 25/999
449/449 [==============================] - 606s 1s/step - loss: 6.0103 - global_pointer_1_loss: 1.6184 - global_pointer_2_loss: 2.2170 - global_pointer_3_loss: 2.1749
f1: 0.62864, precision: 0.65990, recall: 0.60021: : 3585it [01:21, 43.79it/s]
best f1: 0.62864
Epoch 26/999
449/449 [==============================] - 605s 1s/step - loss: 5.7606 - global_pointer_1_loss: 1.5638 - global_pointer_2_loss: 2.1194 - global_pointer_3_loss: 2.0774
f1: 0.62902, precision: 0.65926, recall: 0.60143: : 3585it [01:28, 40.51it/s]
best f1: 0.62902
Epoch 27/999
449/449 [==============================] - 605s 1s/step - loss: 5.4891 - global_pointer_1_loss: 1.4995 - global_pointer_2_loss: 2.0141 - global_pointer_3_loss: 1.9755
f1: 0.63108, precision: 0.66174, recall: 0.60313: : 3585it [01:25, 41.80it/s]
best f1: 0.63108
Epoch 28/999
449/449 [==============================] - 605s 1s/step - loss: 5.2314 - global_pointer_1_loss: 1.4233 - global_pointer_2_loss: 1.9275 - global_pointer_3_loss: 1.8806
f1: 0.63203, precision: 0.66349, recall: 0.60341: : 3585it [01:27, 40.77it/s]
best f1: 0.63203
Epoch 29/999
449/449 [==============================] - 607s 1s/step - loss: 5.0435 - global_pointer_1_loss: 1.3693 - global_pointer_2_loss: 1.8596 - global_pointer_3_loss: 1.8146
f1: 0.63316, precision: 0.66439, recall: 0.60473: : 3585it [01:31, 39.17it/s]
best f1: 0.63316
Epoch 30/999
449/449 [==============================] - 599s 1s/step - loss: 4.8435 - global_pointer_1_loss: 1.3261 - global_pointer_2_loss: 1.7767 - global_pointer_3_loss: 1.7407
f1: 0.63374, precision: 0.66385, recall: 0.60624: : 3585it [01:28, 40.49it/s]
best f1: 0.63374
Epoch 31/999
449/449 [==============================] - 601s 1s/step - loss: 4.6823 - global_pointer_1_loss: 1.2883 - global_pointer_2_loss: 1.7200 - global_pointer_3_loss: 1.6740
f1: 0.63374, precision: 0.66296, recall: 0.60699: : 3585it [01:28, 40.68it/s]
best f1: 0.63374
Epoch 32/999
449/449 [==============================] - 605s 1s/step - loss: 4.4885 - global_pointer_1_loss: 1.2275 - global_pointer_2_loss: 1.6429 - global_pointer_3_loss: 1.6181
f1: 0.63292, precision: 0.66227, recall: 0.60605: : 3585it [01:28, 40.59it/s]
Early stop count 1/5
best f1: 0.63374
Epoch 33/999
449/449 [==============================] - 602s 1s/step - loss: 4.3042 - global_pointer_1_loss: 1.1831 - global_pointer_2_loss: 1.5804 - global_pointer_3_loss: 1.5407
f1: 0.63410, precision: 0.66419, recall: 0.60661: : 3585it [01:23, 42.88it/s]
best f1: 0.63410
Epoch 34/999
449/449 [==============================] - 605s 1s/step - loss: 4.1187 - global_pointer_1_loss: 1.1266 - global_pointer_2_loss: 1.5112 - global_pointer_3_loss: 1.4810
f1: 0.63396, precision: 0.66560, recall: 0.60520: : 3585it [01:33, 38.53it/s]
Early stop count 1/5
best f1: 0.63410
Epoch 35/999
449/449 [==============================] - 598s 1s/step - loss: 3.9684 - global_pointer_1_loss: 1.0939 - global_pointer_2_loss: 1.4542 - global_pointer_3_loss: 1.4203
f1: 0.63395, precision: 0.66433, recall: 0.60624: : 3585it [01:31, 38.99it/s]
Early stop count 2/5
best f1: 0.63410
Epoch 36/999
449/449 [==============================] - 604s 1s/step - loss: 3.8166 - global_pointer_1_loss: 1.0485 - global_pointer_2_loss: 1.4018 - global_pointer_3_loss: 1.3663
f1: 0.63467, precision: 0.66443, recall: 0.60746: : 3585it [01:29, 39.90it/s]
best f1: 0.63467
Epoch 37/999
449/449 [==============================] - 598s 1s/step - loss: 3.6435 - global_pointer_1_loss: 0.9986 - global_pointer_2_loss: 1.3389 - global_pointer_3_loss: 1.3061
f1: 0.63515, precision: 0.66457, recall: 0.60822: : 3585it [01:30, 39.50it/s]
best f1: 0.63515
Epoch 38/999
449/449 [==============================] - 602s 1s/step - loss: 3.5846 - global_pointer_1_loss: 0.9795 - global_pointer_2_loss: 1.3256 - global_pointer_3_loss: 1.2795
f1: 0.63722, precision: 0.66653, recall: 0.61038: : 3585it [01:31, 39.13it/s]
best f1: 0.63722
Epoch 39/999
449/449 [==============================] - 602s 1s/step - loss: 3.4421 - global_pointer_1_loss: 0.9411 - global_pointer_2_loss: 1.2716 - global_pointer_3_loss: 1.2293
f1: 0.63565, precision: 0.66557, recall: 0.60831: : 3585it [01:32, 38.91it/s]
Early stop count 1/5
best f1: 0.63722
Epoch 40/999
449/449 [==============================] - 599s 1s/step - loss: 3.2988 - global_pointer_1_loss: 0.9053 - global_pointer_2_loss: 1.2147 - global_pointer_3_loss: 1.1788
f1: 0.63561, precision: 0.66502, recall: 0.60869: : 3585it [01:35, 37.61it/s]
Early stop count 2/5
best f1: 0.63722
Epoch 41/999
449/449 [==============================] - 599s 1s/step - loss: 3.1723 - global_pointer_1_loss: 0.8681 - global_pointer_2_loss: 1.1746 - global_pointer_3_loss: 1.1296
f1: 0.63549, precision: 0.66464, recall: 0.60878: : 3585it [01:29, 39.87it/s]
Early stop count 3/5
best f1: 0.63722
Epoch 42/999
449/449 [==============================] - 600s 1s/step - loss: 3.0286 - global_pointer_1_loss: 0.8214 - global_pointer_2_loss: 1.1189 - global_pointer_3_loss: 1.0884
f1: 0.63644, precision: 0.66639, recall: 0.60906: : 3585it [01:30, 39.65it/s]
Early stop count 4/5
best f1: 0.63722
Epoch 43/999
449/449 [==============================] - 600s 1s/step - loss: 3.0129 - global_pointer_1_loss: 0.8265 - global_pointer_2_loss: 1.1001 - global_pointer_3_loss: 1.0864
f1: 0.63660, precision: 0.66595, recall: 0.60972: : 3585it [01:33, 38.21it/s]
Early stop count 5/5
Epoch 00042: early stopping THR
best f1: 0.63722
findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.
